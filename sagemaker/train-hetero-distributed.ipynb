{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Notebook for ProcGen Starter Kit with heterogeneous scaling of multiple instances "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n",
    "import boto3\n",
    "\n",
    "from IPython.display import HTML, Markdown\n",
    "from source.common.docker_utils import build_and_push_docker_image\n",
    "from source.common.markdown_helper import generate_help_for_s3_endpoint_permissions, create_s3_endpoint_manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(\"config\", \"sagemaker_config.yaml\")) as f:\n",
    "    sagemaker_config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Amazon SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_session = sagemaker.session.Session()\n",
    "s3_bucket = sagemaker_config[\"S3_BUCKET\"]\n",
    "\n",
    "s3_output_path = 's3://{}/'.format(s3_bucket)\n",
    "print(\"S3 bucket path: {}\".format(s3_output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name_prefix = 'sm-ray-hetero-dist-procgen'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that `local_mode = True` does not work with heterogeneous scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the framework you want to use\n",
    "\n",
    "Set `framework` to `\"tf\"` or `\"torch\"` for tensorflow or pytorch respectively.\n",
    "\n",
    "You will also have to edit your entry point i.e., `train-sagemaker-distributed-gpu.py` with the configuration parameter `\"use_pytorch\"` to match the framework that you have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework = \"tf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your model here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build custom container with procgen installed\n",
    "\n",
    "We build a custom docker container with procgen installed. This takes care of everything:\n",
    "\n",
    "1. Fetching base container image\n",
    "2. Installing procgen and its dependencies\n",
    "3. Uploading the new container image to ECR\n",
    "4. This step can take a long time if you are running on a machine with a slow internet connection. If your notebook instance is in SageMaker or EC2 it should take 3-10 minutes depending on the instance type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build CPU image\n",
    "cpu_repository_short_name = \"sagemaker-procgen-ray-%s\" % \"cpu\"\n",
    "docker_build_args = {\n",
    "    'CPU_OR_GPU': \"cpu\", \n",
    "    'AWS_REGION': boto3.Session().region_name,\n",
    "    'FRAMEWORK': framework\n",
    "}\n",
    "cpu_image_name = build_and_push_docker_image(cpu_repository_short_name, build_args=docker_build_args)\n",
    "print(\"Using CPU ECR image %s\" % cpu_image_name)\n",
    "\n",
    "# Build GPU image\n",
    "gpu_repository_short_name = \"sagemaker-procgen-ray-%s\" % \"gpu\"\n",
    "docker_build_args = {\n",
    "    'CPU_OR_GPU': \"gpu\", \n",
    "    'AWS_REGION': boto3.Session().region_name,\n",
    "    'FRAMEWORK': framework\n",
    "}\n",
    "gpu_image_name = build_and_push_docker_image(gpu_repository_short_name, build_args=docker_build_args)\n",
    "print(\"Using GPU ECR image %s\" % gpu_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train your Ray heterogeneous scaling job here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit the training code\n",
    "\n",
    "The training code is written in the file `train-sagemaker-distributed-gpu.py` which is uploaded in the /source directory.\n",
    "\n",
    "*Note that ray will automatically set `\"ray_num_cpus\"` and `\"ray_num_gpus\"` in `_get_ray_config`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pygmentize source/train-sagemaker-distributed-gpu.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the RL model using the Python SDK Script mode\n",
    "\n",
    "If you are using local mode, the training will run on the notebook instance. When using SageMaker for training, you can select a GPU or CPU instance. The RLEstimator is used for training RL jobs.\n",
    "\n",
    "1. Specify the source directory where the environment, presets and training code is uploaded.\n",
    "2. Specify the entry point as the training code\n",
    "3. Specify the custom image to be used for the training environment.\n",
    "4. Define the training parameters such as the instance count, job name, S3 path for output and job name.\n",
    "5. Define the metrics definitions that you are interested in capturing in your logs. These can also be visualized in CloudWatch and SageMaker Notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions =  [\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episodes_total', 'Regex': 'episodes_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'num_steps_trained', 'Regex': 'num_steps_trained: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'timesteps_total', 'Regex': 'timesteps_total: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "    {'Name': 'training_iteration', 'Regex': 'training_iteration: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "\n",
    "    {'Name': 'episode_reward_max', 'Regex': 'episode_reward_max: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_mean', 'Regex': 'episode_reward_mean: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'}, \n",
    "    {'Name': 'episode_reward_min', 'Regex': 'episode_reward_min: ([-+]?[0-9]*[.]?[0-9]+([eE][-+]?[0-9]+)?)'},\n",
    "] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To scale out RL training, we can increase the number of rollout workers. However, with more rollouts, training can often become the bottleneck. To prevent this, we can use an instance with one or more GPUs for training, and multiple CPU instances for rollouts.\n",
    "\n",
    "Since SageMaker supports a single type of instance in a training job, we can achieve the above by spinning two SageMaker jobs and letting them communicate with each other. For the sake of naming, we'll use Primary cluster to refer to 1 or more GPU instances, and Secondary cluster to refer to the cluster of CPU instances.\n",
    "\n",
    "> Please note that local_mode cannot be used for testing this type of scaling.\n",
    "\n",
    "Before we configure the SageMaker job, let us first ensure that we run SageMaker in VPC mode. VPC mode will allow the two SageMaker jobs to communicate over network.\n",
    "\n",
    "This can be done by supplying subnets and security groups to the job launching scripts. We will use the default VPC configuration for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.client('ec2')\n",
    "default_vpc = [vpc['VpcId'] for vpc in ec2.describe_vpcs()['Vpcs'] if vpc[\"IsDefault\"] == True][0]\n",
    "\n",
    "default_security_groups = [group[\"GroupId\"] for group in ec2.describe_security_groups()['SecurityGroups'] \\\n",
    "                   if group[\"GroupName\"] == \"default\" and group[\"VpcId\"] == default_vpc]\n",
    "\n",
    "default_subnets = [subnet[\"SubnetId\"] for subnet in ec2.describe_subnets()[\"Subnets\"] \\\n",
    "                  if subnet[\"VpcId\"] == default_vpc and subnet['DefaultForAz']==True]\n",
    "\n",
    "print(\"Using default VPC:\", default_vpc)\n",
    "print(\"Using default security group:\", default_security_groups)\n",
    "print(\"Using default subnets:\", default_subnets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SageMaker job running in VPC mode cannot access S3 resources. So, we need to create a VPC S3 endpoint to allow S3 access from SageMaker container. To learn more about the VPC mode, please visit [this link](https://docs.aws.amazon.com/sagemaker/latest/dg/train-vpc.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_region = boto3.Session().region_name\n",
    "try:\n",
    "    route_tables = [route_table[\"RouteTableId\"] for route_table in ec2.describe_route_tables()['RouteTables']\\\n",
    "                if route_table['VpcId'] == default_vpc]\n",
    "except Exception as e:\n",
    "    if \"UnauthorizedOperation\" in str(e):\n",
    "        display(Markdown(generate_help_for_s3_endpoint_permissions(role)))\n",
    "    else:\n",
    "        display(Markdown(create_s3_endpoint_manually(aws_region, default_vpc)))\n",
    "    raise e\n",
    "\n",
    "print(\"Trying to attach S3 endpoints to the following route tables:\", route_tables)\n",
    "\n",
    "assert len(route_tables) >= 1, \"No route tables were found. Please follow the VPC S3 endpoint creation \"\\\n",
    "                              \"guide by clicking the above link.\"\n",
    "\n",
    "try:\n",
    "    ec2.create_vpc_endpoint(DryRun=False,\n",
    "                           VpcEndpointType=\"Gateway\",\n",
    "                           VpcId=default_vpc,\n",
    "                           ServiceName=\"com.amazonaws.{}.s3\".format(aws_region),\n",
    "                           RouteTableIds=route_tables)\n",
    "    print(\"S3 endpoint created successfully!\")\n",
    "except Exception as e:\n",
    "    if \"RouteAlreadyExists\" in str(e):\n",
    "        print(\"S3 endpoint already exists.\")\n",
    "    elif \"UnauthorizedOperation\" in str(e):\n",
    "        display(Markdown(generate_help_for_s3_endpoint_permissions(role)))\n",
    "        raise e\n",
    "    else:\n",
    "        display(Markdown(create_s3_endpoint_manually(aws_region, default_vpc)))\n",
    "        raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure instance types\n",
    "\n",
    "Let us configure a cluster with 1 Volta (V100) GPU and 40 CPU cores. We can do this by using 1 ml.p3.2xlarge instance and 2 ml.c5.4xlarge instances, since ml.p3.2xlarge has 8 CPU cores and ml.c5.4xlarge has 16 CPU cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_cluster_instance_type = sagemaker_config[\"GPU_TRAINING_INSTANCE\"]\n",
    "primary_cluster_instance_count = 1\n",
    "\n",
    "secondary_cluster_instance_type = sagemaker_config[\"CPU_TRAINING_INSTANCE\"]\n",
    "secondary_cluster_instance_count = 2\n",
    "\n",
    "total_cpus = 40 - 1 # Leave one for ray scheduler\n",
    "total_gpus = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For heterogeneous training, we also pass some additional parameters to the training job that aid in synchronization across instances:\n",
    "\n",
    "* `s3_bucket`, `s3_prefix`: Used for storing metadata like master IP address\n",
    "* `rl_cluster_type`: \"primary\" or \"secondary\"\n",
    "* `aws_region`: This is required for making connection to S3 in VPC mode\n",
    "* `rl_num_instances_secondary`: Number of nodes in secondary cluster\n",
    "* `subnets`, `security_group_ids`: Required by VPC mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We explicitly need to specify these params so that the two jobs can synchronize using the metadata stored here\n",
    "s3_prefix = \"hetero-training-job\"\n",
    "\n",
    "# Make sure that the prefix is empty\n",
    "!aws s3 rm --recursive s3://{s3_bucket}/{s3_prefix}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for internal testing: If you kick-start another heterogenous training job, make sure to either run tha above cell directly or rename the s3_prefix and re-run the above cell. Otherwise metadata in s3 bucket wiill be incorrect and instances in the new job cannot join the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch primary cluster (1 GPU training instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for internal testing: Make sure at least one gpu is required in rllib config (`exp{'Config':{'num_gpus'}}`). Otherwise the Trainer() will be placed on secondary instance, leading to checkpoint restoration failure during model export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \"coinrun\"\n",
    "\n",
    "primary_cluster_estimator = RLEstimator(entry_point=\"train-sagemaker-distributed-gpu.py\",\n",
    "                            source_dir='source',\n",
    "                            dependencies=[\"source/utils\", \"source/common/\", \n",
    "                                          \"neurips2020-procgen-starter-kit/\"],\n",
    "                            image_name=gpu_image_name,\n",
    "                            role=role,\n",
    "                            train_instance_type=primary_cluster_instance_type,\n",
    "                            train_instance_count=primary_cluster_instance_count,\n",
    "                            output_path=s3_output_path,\n",
    "                            base_job_name=job_name_prefix,\n",
    "                            metric_definitions=metric_definitions,\n",
    "                            train_max_run=int(3600 * .5), # Maximum runtime in seconds\n",
    "                            hyperparameters={\n",
    "                                \"s3_prefix\": s3_prefix, # Important for syncing\n",
    "                                \"s3_bucket\": s3_bucket, # Important for syncing\n",
    "                                \"aws_region\": boto3.Session().region_name, # Important for S3 connection\n",
    "                                \"rl_cluster_type\": \"primary\", # Important for syncing\n",
    "                                \"rl_num_instances_secondary\": secondary_cluster_instance_count, # Important for syncing\n",
    "                                \n",
    "                                #\"rl.training.upload_dir\": s3_output_path,\n",
    "                                \"rl.training.config.env_config.env_name\": env,\n",
    "                                \n",
    "                                #\"rl.training.config.num_workers\": total_cpus,\n",
    "                                #\"rl.training.config.train_batch_size\": 20000,\n",
    "                                #\"rl.training.config.num_gpus\": total_gpus,\n",
    "                            },\n",
    "                            subnets=default_subnets, # Required for VPC mode\n",
    "                            security_group_ids=default_security_groups # Required for VPC mode\n",
    "                        )\n",
    "\n",
    "primary_cluster_estimator.fit(wait=False)\n",
    "primary_job_name = primary_cluster_estimator.latest_training_job.job_name\n",
    "print(\"Primary Training job: %s\" % primary_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch secondary cluster (2 CPU instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary_cluster_estimator = RLEstimator(entry_point=\"train-sagemaker-distributed-gpu.py\",\n",
    "                            source_dir='source',\n",
    "                            dependencies=[\"source/utils\", \"source/common/\", \n",
    "                                          \"neurips2020-procgen-starter-kit/\"],\n",
    "                            image_name=cpu_image_name,\n",
    "                            role=role,\n",
    "                            train_instance_type=secondary_cluster_instance_type,\n",
    "                            train_instance_count=secondary_cluster_instance_count,\n",
    "                            output_path=s3_output_path,\n",
    "                            base_job_name=job_name_prefix,\n",
    "                            metric_definitions=metric_definitions,\n",
    "                            train_max_run=3600, # Maximum runtime in seconds\n",
    "                            hyperparameters={\n",
    "                                \"s3_prefix\": s3_prefix, # Important for syncing\n",
    "                                \"s3_bucket\": s3_bucket, # Important for syncing\n",
    "                                \"aws_region\": boto3.Session().region_name, # Important for S3 connection\n",
    "                                \"rl_cluster_type\": \"secondary\", # Important for syncing\n",
    "                                \n",
    "                                #\"rl.training.upload_dir\": s3_output_path,\n",
    "                                \"rl.training.config.env_config.env_name\": env,\n",
    "                            },\n",
    "                            subnets=default_subnets,\n",
    "                            security_group_ids=default_security_groups # Required for VPC mode\n",
    "                        )\n",
    "\n",
    "secondary_cluster_estimator.fit(wait=False)\n",
    "secondary_job_name = secondary_cluster_estimator.latest_training_job.job_name\n",
    "print(\"Secondary Training job: %s\" % secondary_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}